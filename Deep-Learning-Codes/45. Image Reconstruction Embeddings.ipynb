{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNe7t/haRLN5evRqu9Hgb+K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import necessary libraries"],"metadata":{"id":"gwmaitYdx-_Z"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torchvision.utils import save_image, make_grid\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import requests\n","from io import BytesIO"],"metadata":{"id":"DEHIfTh3x_Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load and preprocess the image from URL"],"metadata":{"id":"41IH7gYdx_5P"}},{"cell_type":"code","source":["def load_image_from_url(url, transform):\n","    response = requests.get(url)\n","    image = Image.open(BytesIO(response.content)).convert('RGB')\n","    return transform(image).unsqueeze(0)"],"metadata":{"id":"Q6t2GIkGyAKt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the transformation pipeline"],"metadata":{"id":"aIMx34StyApK"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resizing the image to 224x224\n","    transforms.ToTensor(),  # Converting the image to Tensor\n","])"],"metadata":{"id":"V8z0gnu9yA37"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the pre-trained VGG19 model"],"metadata":{"id":"8efupNjeyBDL"}},{"cell_type":"markdown","source":["Extract Feature Embeddings\n","\n","    - Feature embeddings are high-dimensional representations of the image\n","    - They capture hierarchical information: edges → textures → objects\n","    - Each layer's output is stored for visualization"],"metadata":{"id":"U5k1TMv-3ZFj"}},{"cell_type":"code","source":["vgg = models.vgg19(pretrained=True).features.eval()\n","\n","# Extract embeddings from each layer\n","def get_all_embeddings(image, model):\n","    embeddings = {}\n","    for i, layer in enumerate(model):\n","        image = layer(image)\n","        embeddings[f'Layer {i}'] = image.detach()\n","    return embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czLRGNMdyBO1","executionInfo":{"status":"ok","timestamp":1742746588548,"user_tz":-330,"elapsed":8703,"user":{"displayName":"AYAN PAUL","userId":"11246289557973384086"}},"outputId":"0620c421-cac4-4bcd-83c3-cd1b6c2a91ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:06<00:00, 89.2MB/s]\n"]}]},{"cell_type":"markdown","source":["Visualization function for embeddings"],"metadata":{"id":"_RA9E_7TyBZq"}},{"cell_type":"markdown","source":["Visualizing Feature Embeddings\n","\n","    - Each feature map consists of multiple channels (depth = number of filters)\n","    - We compute the **mean activation per channel** to visualize how each layer processes the image\n","    - Mathematically, the mean activation of a layer `l` is:\n","      V^l = (1/C) * sum(E^l_c)   where C = number of channels in layer l"],"metadata":{"id":"hyQeFlz73llN"}},{"cell_type":"code","source":["def visualize_embeddings(embeddings):\n","    for layer, embedding in embeddings.items():\n","        plt.figure(figsize=(10, 5))\n","        plt.imshow(embedding[0].mean(0).cpu().numpy(), cmap='viridis')\n","        plt.title(f'{layer} Embedding Visualization')\n","        plt.colorbar()\n","        plt.show()\n","\n","# Load and process the original image\n","animal_url = \"https://upload.wikimedia.org/wikipedia/commons/7/73/Lion_waiting_in_Namibia.jpg\"\n","original_image = load_image_from_url(animal_url, transform)\n","\n","# Extract embeddings from all layers\n","embeddings = get_all_embeddings(original_image, vgg)\n","\n","# Visualize embeddings\n","visualize_embeddings(embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"19zNi_juJyy1IYrLHGeegwqQ90QgiDCso"},"id":"oGPxIpO0yBkx","executionInfo":{"status":"ok","timestamp":1742746615306,"user_tz":-330,"elapsed":10627,"user":{"displayName":"AYAN PAUL","userId":"11246289557973384086"}},"outputId":"c967b48e-af88-4066-8e3d-ed62d123c39c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Summary of the Code: Image Reconstruction Using Embeddings\n","\n","    This code extracts feature embeddings from a pre-trained VGG19 model and visualizes them.\n","    Feature embeddings represent high-level features of an image extracted at different layers of a CNN."],"metadata":{"id":"ifhagxdS2-Fc"}},{"cell_type":"markdown","source":["Outcome:\n","\n","    Helps understand how CNN layers process images at different levels.\n","\n","    Useful for feature visualization & interpretability in deep learning."],"metadata":{"id":"FdeD64tK3DE1"}}]}